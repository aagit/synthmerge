endpoints:

  - name: "Claude Opus 4.6"
    url: "https://api.anthropic.com/v1/messages"
    type: "anthropic"
    x_api_key_file: "~/.keys/anthropic.api-key"
    json:
      max_tokens: 20000
      model: "claude-opus-4-6"
      temperature: 0
    headers:
      anthropic-version: "2023-06-01"

  - name: "Claude Sonnet 4.5"
    url: "https://api.anthropic.com/v1/messages"
    type: "anthropic"
    x_api_key_file: "~/.keys/anthropic.api-key"
    json:
      max_tokens: 20000
      model: "claude-sonnet-4-5"
      temperature: 0
    headers:
      anthropic-version: "2023-06-01"

  - name: "Vertex Claude Opus 4.6"
    url: "https://aiplatform.googleapis.com/v1/projects/<project_id>/locations/global/publishers/anthropic/models/claude-opus-4-6@default:streamRawPredict"
    type: "anthropic"
    # gcloud auth print-access-token >~/.keys/vertex-ai.api-key
    api_key_file: "~/.keys/vertex-ai.api-key"
    max_delay: 10000
    json:
      anthropic_version: "vertex-2023-10-16"
      max_tokens: 20000
      temperature: 0

  - name: "Vertex Claude Sonnet 4.5"
    url: "https://aiplatform.googleapis.com/v1/projects/<project_id>/locations/global/publishers/anthropic/models/claude-sonnet-4-5@20250929:streamRawPredict"
    type: "anthropic"
    # gcloud auth print-access-token >~/.keys/vertex-ai.api-key
    api_key_file: "~/.keys/vertex-ai.api-key"
    max_delay: 10000
    json:
      anthropic_version: "vertex-2023-10-16"
      max_tokens: 20000
      temperature: 0

  - name: "Vertex Gateway Claude Sonnet 4.0"
    url: "https://host/path"
    type: "anthropic"
    api_key_file: "~/.keys/claude.api-key"
    json:
      anthropic_version: "something-YYYY-MM-DD"
      max_tokens: 20000
      temperature: 0
    # Optional root certificate for HTTPS endpoints
    # root_certificate_pem: "~/.ssl/corp-ca.pem"

  - name: "Patchpal AI"
    type: "patchpal"
    url: "http://patchpal.usersys.redhat.com:9080/v1"
    #n_beams: 3

  - name: "Gemini 3 Flash Preview"
    url: "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions"
    type: "openai"
    api_key_file: "~/.keys/gemini.api-key"
    json:
      model: "gemini-3-flash-preview"
      reasoning_effort: "none"

  - name: "llama.cpp vulkan minimal"
    url: "http://localhost:8811/v1/chat/completions"
    type: "openai"
    gbnf: true

  - name: "llama.cpp vulkan"
    url: "http://localhost:8811/v1/chat/completions"
    #timeout: 600000
    #retries: 10
    #delay: 1000
    #max_delay: 600000
    #wait: 1000
    type: "openai"
    gbnf: true
    json:
      temperature: 0.15

      #temperature: 0.7
      #top_p: 0.8
      #top_k: 20
      #min_p: 0.01

      # n_probs: 1 provides the probability of the lowest probability
      # token in the resolved conflict
      n_probs: 1

      # n_probs: 2 same as n_probs: 1 but it also provides two more
      # beams with the perplexity search algorithm of synthmerge
      # applied to the logprobs, which is a client side only
      # approximated beam search
      #n_probs: 2
    variants:
      # one query for each entry in the variants list
      - name: "default"
      - name: "no_diff"
        context:
          no_diff: true
      #- name: "min_p"
      #  json:
      #    temperature: 0.3
      #    top_p: 1.0
      #    top_k: 0
      #    min_p: 0.9

  - name: "llama.cpp vulkan no_chat"
    url: "http://localhost:8811/v1/completions"
    type: "openai"
    no_chat: true
    gbnf: true
    context:
      no_training: true
